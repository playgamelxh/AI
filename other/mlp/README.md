2.1 多层感知器（Multi-Layer Perceptron，简称MLP）

这是一种经典的神经网络方法。MLP本身并不是超级强大，但你会发现它们几乎集成在任何其他架构中(令人惊讶的是，甚至在transformer中)。MLP基本上是一个线性层序列或完全连接的层。
[图片]
在AI社区找到各种模式的最佳架构之前，MLP长期以来一直用于建模不同类型的数据，但有一点是肯定的，它们不适合序列建模。由于它们的前馈设计，它们不能保持序列中信息的顺序。当数据的顺序丢失时，序列数据就失去了意义。因此，MLP不能保持信息的顺序使其不适合序列建模。此外，MLP需要许多参数，这是神经网络可能具有的另一个不希望的属性。

多层感知器（Multi-Layer Perceptron，简称 MLP）是一种基础的人工神经网络（ANN）结构，属于前馈神经网络的核心类型，也是深度学习的 “入门级” 模型。它通过堆叠多层神经元（感知器），解决了单层感知器无法处理的非线性问题（如异或逻辑），是后续复杂模型（如 CNN、Transformer）的基础组件。

一、MLP 的核心定义与本质
MLP 的本质是 “多层感知器的堆叠”，核心特点是：
- 分层结构：包含输入层、至少一层隐藏层、输出层；
- 全连接：相邻层的神经元之间两两连接（即上一层每个神经元的输出会作为下一层所有神经元的输入）；
- 非线性激活：隐藏层和输出层需通过激活函数引入非线性，否则多层结构等价于单层感知器（无法拟合非线性数据）。
可以类比为 “多层逻辑回归的组合”—— 每一层都是对前一层信号的 “加权变换 + 非线性转换”，最终通过输出层给出预测结果。

二、MLP 的结构组成（分层解析）
以 “手写数字识别（MNIST）” 任务为例（输入为 28×28 像素图像，输出为 10 个数字类别），MLP 的结构如下：
1. 输入层（Input Layer）：接收原始数据
- 作用：将原始数据转换为神经网络可处理的 “向量信号”，不进行任何计算；
- 神经元数量：等于输入数据的维度（如 MNIST 图像展平后为 784 个像素，故输入层有 784 个神经元）；
- 特点：无激活函数，直接传递原始输入值（如像素灰度值 0~255）。
2. 隐藏层（Hidden Layer）：提取数据特征
- 作用：通过多层 “加权求和 + 非线性激活”，逐步从原始数据中提取抽象特征（如第一层提取边缘、纹理，深层提取形状、部件）；
- 层数与神经元数量：
  - 层数：至少 1 层（否则为单层感知器），常见 1~5 层（过多易过拟合）；
  - 神经元数量：需根据任务调整（如 MNIST 任务可设 128、256 个神经元），通常越多表示拟合能力越强，但计算成本越高；
- 核心计算：对每个神经元，先计算前一层输入的加权和，再通过激活函数输出结果：$$z_i=∑_jw_{ij}⋅x_j+b_i$$其中：
  - $$x_j$$：前一层神经元的输出；
  - $$w_{ij}$$：连接前一层第 j 个神经元与当前层第 i 个神经元的权重（核心参数，通过训练优化）；
  - $$b_i$$：当前层第 i 个神经元的偏置（避免输入全为 0 时无法激活）；
  - $$f(⋅)$$：激活函数（如 ReLU、Sigmoid），引入非线性。
3. 输出层（Output Layer）：输出预测结果
- 作用：将隐藏层提取的特征映射到 “任务目标空间”，给出最终预测；
- 神经元数量：等于任务的输出维度（如 MNIST 10 分类任务，输出层有 10 个神经元）；
- 激活函数选择（与任务类型强相关）：
  - 分类任务：
    - 二分类：1 个神经元 + Sigmoid 激活（输出 0~1 概率）；
    - 多分类（互斥）：N 个神经元 + Softmax 激活（输出每个类别的概率，总和为 1）；
  - 回归任务：1 个神经元 + 无激活（或 Linear 激活），直接输出连续值。

三、MLP 的核心机制：激活函数与训练过程
1. 激活函数：引入非线性的关键
没有激活函数时，MLP 无论多少层，最终都等价于 “输入的线性组合”（无法拟合非线性数据，如异或）。常用激活函数：
激活函数
公式
特点
适用场景
ReLU（修正线性单元）
$$f(z)=max(0,z)$$
计算快、缓解梯度消失，是隐藏层首选
隐藏层（主流选择）
Sigmoid $$ f(z)=\frac {1} {1+e^{−z}} $$
输出 0~1（可表示概率），但易梯度消失
二分类输出层、早期隐藏层
Softmax
$$f(z_i)=\frac {e^{z_i}} {∑_ke^{z_k}}$$
输出多类别概率（总和 1）
多分类输出层
Tanh
$$f(z)=\frac {e^z-e^{-z}} {e^z+e^{-z}}$$
输出 -1~1，比 Sigmoid 更易训练
早期隐藏层（现少用）
2. 训练过程：梯度下降优化参数
MLP 的训练核心是 “通过梯度下降最小化预测值与真实值的误差”，分为 4 个步骤：
（1）前向传播（Forward Propagation）
从输入层到输出层，逐层计算每个神经元的输出，最终得到模型的预测结果（如分类任务的概率分布）。
（2）计算损失（Loss Calculation）
通过损失函数衡量预测结果与真实标签的差距（损失值越小，模型越优）：
- 分类任务：交叉熵损失（Cross-Entropy Loss）；
- 回归任务：均方误差（MSE，Mean Squared Error）。
（3）反向传播（Backward Propagation）
从输出层反向计算 “每个参数（权重 w、偏置 b）对损失的梯度”（即参数变化如何影响损失），核心是链式法则（Chain Rule）—— 梯度从输出层传递到输入层，逐层更新参数的梯度。
（4）参数更新（Parameter Update）
根据反向传播得到的梯度，通过优化器调整权重和偏置，降低损失：
- 基础优化器：随机梯度下降（SGD）；
- 进阶优化器：Adam、RMSprop（自适应学习率，训练更稳定）。
重复以上 4 步，直到损失收敛（或达到最大迭代次数），模型训练完成。

四、MLP 的优缺点与适用场景

优点：
1. 结构简单：易于理解和实现，是入门深度学习的最佳模型；
2. 非线性拟合能力：通过隐藏层和激活函数，可拟合复杂的非线性数据；
3. 泛化能力：在小规模数据集（如 MNIST、Iris）上表现优异，且计算成本低。

缺点：
1. 全连接导致参数过多：输入维度高时（如 1000×1000 图像），权重矩阵规模爆炸（如输入层 1e6 神经元 + 隐藏层 1e3 神经元，权重数量达 1e9），易过拟合且计算缓慢；
2. 缺乏空间 / 时序特征提取能力：对图像（需空间特征，如边缘）、文本（需时序特征，如语序）等数据，无法像 CNN、RNN 那样高效提取结构化特征；
3. 易过拟合：隐藏层过多或神经元数量过大时，模型易 “死记” 训练数据，在新数据上表现差（需通过 dropout、正则化缓解）。

适用场景：
- 小规模数据集任务：如 Iris 分类、简单回归（如房价预测简化版）；
- 特征已结构化的数据：如表格数据（Excel 数据，每行是样本，每列是特征）；
- 作为复杂模型的组件：如 Transformer 的 Feed-Forward Network（本质是两层 MLP）、CNN 的全连接输出层。