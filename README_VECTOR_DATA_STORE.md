## Vector Data Store
#### 一、数据存储：向量数据库是核心
向量化后的分片数据（通常包含 “文本片段 + 对应向量 + 元数据”）需要专门的存储系统，向量数据库是主流选择，因为它们针对高维向量的存储、索引和相似度搜索做了深度优化。常见方案包括：
1. 专业向量数据库（推荐）
   这类数据库专为向量检索设计，支持高效的相似度计算和大规模数据存储，适合生产环境。
    - Milvus：开源分布式向量数据库，支持十亿级向量存储，兼容多种相似度算法（余弦、欧氏距离等），适合高并发场景
    - Qdrant：开源轻量级向量数据库，支持向量 + 元数据过滤，部署简单（可单机可分布式），适合中小规模数据
    - Pinecone：云原生托管向量数据库，无需关心部署和维护，支持自动扩缩容，适合快速上手的企业级场景。
    - Weaviate：开源向量数据库，内置语义搜索能力，支持 GraphQL/REST API，可结合元数据做复杂过滤。
    - Chroma：轻量级开源向量数据库，API 简单易用，适合开发测试或小规模应用（如本地知识库）。
2. 传统数据库 + 向量插件（过渡方案）
   如果已有传统数据库（如关系型、文档型），可通过插件扩展向量存储能力，适合数据量不大或需要与现有系统兼容的场景。
    - PostgreSQL + pgvector：PostgreSQL 的向量扩展插件，支持向量存储和相似度搜索（余弦、内积等），适合已有 PostgreSQL 生态的场景。
    - MongoDB 5.0+：原生支持向量字段和向量索引，可结合文档存储（元数据）和向量检索，适合文档型数据为主的场景。
3. 本地文件（仅适合开发测试）
   小规模数据（如几百到几千个分片）可临时存储在本地文件（如 JSON、CSV），但不适合高并发或大规模检索。
   例如：用 JSON 文件存储 {"text": "分片内容", "vector": [0.1, 0.2, ...], "metadata": {"source": "文档A"}}。
#### 二、检索逻辑：向量相似度是核心
检索的目标是从存储的向量中，快速找到与 “用户查询向量” 最相似的 Top N 个文本分片，核心流程如下：
1. 检索前：查询向量化
   用户输入的自然语言查询（如 “如何配置 RAG 的向量数据库？”），需先用与文档分片相同的嵌入模型（如 OpenAI Embedding、BGE-M3）转换为向量（与文档向量维度一致，通常是 768 维、1536 维等）。
2. 核心：向量相似度搜索
   通过向量数据库的 API，传入 “查询向量”，计算其与数据库中所有文档向量的相似度，返回相似度最高的前 K 个结果（通常 K=3~10，可根据效果调整）。
   常用的相似度算法：
   - 余弦相似度：衡量向量方向的一致性（忽略长度），适合文本语义匹配（最常用）。
   - 欧氏距离：衡量向量空间中的直线距离，适合高维向量场景。
   - 内积：衡量向量的投影重叠度，在归一化向量后与余弦相似度效果一致。
3. 优化：提升检索准确性和效率
   - 元数据过滤：结合分片的元数据（如文档来源、时间、类别）做前置过滤，缩小检索范围。例如：只检索 “2023 年后的技术文档”。
   - 混合检索（Hybrid Search）：结合 “向量语义检索” 和 “关键词检索”（如 BM25 算法），避免纯向量检索漏掉关键词匹配的重要内容。
   - 索引优化：向量数据库通过索引（如 HNSW、IVF-Flat）加速检索，减少计算量。例如：HNSW 索引适合高维向量的快速近似搜索。
   - 重排序（Reranking）：对初步返回的 Top K 结果，用更精细的模型（如 CrossEncoder）重新排序，进一步提升相关性。
#### 三、示例流程（以 Milvus 为例）
1. 数据入库：
将分片后的文本、向量、元数据（如source、chunk_id）写入 Milvus 集合（Collection），并创建向量索引（如 HNSW）。
2. 用户查询：
   用户输入 “RAG 如何选择向量数据库？”，用 BGE-M3 模型转换为向量 query_vec。
3. 检索：
   调用 Milvus 的search接口，传入query_vec，设置相似度算法为余弦，返回 Top 5 相似的分片向量及对应的文本。
4. 结果处理：
   将返回的文本分片拼接，作为上下文传入 LLM 生成最终回答。
#### 总结
- 存储：优先选择专业向量数据库（如 Milvus、Qdrant），兼顾性能和扩展性；小规模场景可用 PostgreSQL+pgvector 或本地文件。
- 检索：核心是 “查询向量化→向量相似度搜索→Top K 结果返回”，通过元数据过滤、混合检索、重排序等优化提升效果。

选择方案时需结合数据量（百万级以上优先分布式向量库）、实时性（高并发选托管方案如 Pinecone）、成本（开源方案适合自建）等因素。

#### FLOAT_VECTOR（稠密向量）和 SPARSE_FLOAT_VECTOR（稀疏向量）
1. 优先选择 FLOAT_VECTOR（稠密向量）的场景
   * 向量维度适中（通常在 128~2048 维之间），且非零值占比高（例如超过 30%）。
   * 由深度学习模型生成，如：
      + 文本：BERT、GPT、Sentence-BERT 等预训练模型的输出（固定维度，几乎所有维度都有非零值）。
      + 图像：ResNet、VGG 等模型的特征向量（稠密表示图像特征）。
      + 音频：MFCC 特征、语音识别模型的嵌入向量。
   * 需要高频相似度计算（如实时检索），稠密向量的紧凑存储形式能提升计算效率。
2. 优先选择 SPARSE_FLOAT_VECTOR（稀疏向量）的场景
   * 向量维度极高（通常超过 1 万维），且非零值占比极低（例如低于 1%）。
   * 由传统统计或离散特征生成，如：
      + 文本：TF-IDF、词袋模型（BoW）、One-Hot 编码（维度等于词汇表大小，通常几十万维，仅少数词对应非零值）。
      + 推荐系统：用户 - 物品交互矩阵（用户数 / 物品数极大，大多数用户未交互过大多数物品）。
      + 特征工程：高维离散特征（如类别型特征的哈希编码）。
   * 对存储空间敏感，稀疏向量通过只存储非零值，可大幅减少存储开销（例如 100 万维的稀疏向量，若仅 100 个非零值，存储量远小于稠密向量）。
3. 特殊情况的判断
   * 如果向量维度超过 2048 但非零值较多（如 1 万维中 30% 是非零值），此时稀疏向量的存储优势不明显，反而可能因索引开销降低效率，建议用稠密向量。
   * 如果是混合特征（部分稠密 + 部分稀疏），可拆分存储（稠密部分用 FLOAT_VECTOR，稀疏部分用 SPARSE_FLOAT_VECTOR），或通过模型将稀疏特征转为稠密表示（如通过嵌入层）。

