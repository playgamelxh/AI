## Vector Data Store
#### 一、数据存储：向量数据库是核心
向量化后的分片数据（通常包含 “文本片段 + 对应向量 + 元数据”）需要专门的存储系统，向量数据库是主流选择，因为它们针对高维向量的存储、索引和相似度搜索做了深度优化。常见方案包括：
1. 专业向量数据库（推荐）
   这类数据库专为向量检索设计，支持高效的相似度计算和大规模数据存储，适合生产环境。
    - Milvus：开源分布式向量数据库，支持十亿级向量存储，兼容多种相似度算法（余弦、欧氏距离等），适合高并发场景
    - Qdrant：开源轻量级向量数据库，支持向量 + 元数据过滤，部署简单（可单机可分布式），适合中小规模数据
    - Pinecone：云原生托管向量数据库，无需关心部署和维护，支持自动扩缩容，适合快速上手的企业级场景。
    - Weaviate：开源向量数据库，内置语义搜索能力，支持 GraphQL/REST API，可结合元数据做复杂过滤。
    - Chroma：轻量级开源向量数据库，API 简单易用，适合开发测试或小规模应用（如本地知识库）。
2. 传统数据库 + 向量插件（过渡方案）
   如果已有传统数据库（如关系型、文档型），可通过插件扩展向量存储能力，适合数据量不大或需要与现有系统兼容的场景。
    - PostgreSQL + pgvector：PostgreSQL 的向量扩展插件，支持向量存储和相似度搜索（余弦、内积等），适合已有 PostgreSQL 生态的场景。
    - MongoDB 5.0+：原生支持向量字段和向量索引，可结合文档存储（元数据）和向量检索，适合文档型数据为主的场景。
3. 本地文件（仅适合开发测试）
   小规模数据（如几百到几千个分片）可临时存储在本地文件（如 JSON、CSV），但不适合高并发或大规模检索。
   例如：用 JSON 文件存储 {"text": "分片内容", "vector": [0.1, 0.2, ...], "metadata": {"source": "文档A"}}。
#### 二、检索逻辑：向量相似度是核心
检索的目标是从存储的向量中，快速找到与 “用户查询向量” 最相似的 Top N 个文本分片，核心流程如下：
1. 检索前：查询向量化
   用户输入的自然语言查询（如 “如何配置 RAG 的向量数据库？”），需先用与文档分片相同的嵌入模型（如 OpenAI Embedding、BGE-M3）转换为向量（与文档向量维度一致，通常是 768 维、1536 维等）。
2. 核心：向量相似度搜索
   通过向量数据库的 API，传入 “查询向量”，计算其与数据库中所有文档向量的相似度，返回相似度最高的前 K 个结果（通常 K=3~10，可根据效果调整）。
   常用的相似度算法：
   - 余弦相似度：衡量向量方向的一致性（忽略长度），适合文本语义匹配（最常用）。
   - 欧氏距离：衡量向量空间中的直线距离，适合高维向量场景。
   - 内积：衡量向量的投影重叠度，在归一化向量后与余弦相似度效果一致。
3. 优化：提升检索准确性和效率
   - 元数据过滤：结合分片的元数据（如文档来源、时间、类别）做前置过滤，缩小检索范围。例如：只检索 “2023 年后的技术文档”。
   - 混合检索（Hybrid Search）：结合 “向量语义检索” 和 “关键词检索”（如 BM25 算法），避免纯向量检索漏掉关键词匹配的重要内容。
   - 索引优化：向量数据库通过索引（如 HNSW、IVF-Flat）加速检索，减少计算量。例如：HNSW 索引适合高维向量的快速近似搜索。
   - 重排序（Reranking）：对初步返回的 Top K 结果，用更精细的模型（如 CrossEncoder）重新排序，进一步提升相关性。
#### 三、示例流程（以 Milvus 为例）
1. 数据入库：
将分片后的文本、向量、元数据（如source、chunk_id）写入 Milvus 集合（Collection），并创建向量索引（如 HNSW）。
2. 用户查询：
   用户输入 “RAG 如何选择向量数据库？”，用 BGE-M3 模型转换为向量 query_vec。
3. 检索：
   调用 Milvus 的search接口，传入query_vec，设置相似度算法为余弦，返回 Top 5 相似的分片向量及对应的文本。
4. 结果处理：
   将返回的文本分片拼接，作为上下文传入 LLM 生成最终回答。
#### 总结
- 存储：优先选择专业向量数据库（如 Milvus、Qdrant），兼顾性能和扩展性；小规模场景可用 PostgreSQL+pgvector 或本地文件。
- 检索：核心是 “查询向量化→向量相似度搜索→Top K 结果返回”，通过元数据过滤、混合检索、重排序等优化提升效果。

选择方案时需结合数据量（百万级以上优先分布式向量库）、实时性（高并发选托管方案如 Pinecone）、成本（开源方案适合自建）等因素。

#### FLOAT_VECTOR（稠密向量）和 SPARSE_FLOAT_VECTOR（稀疏向量）
1. 优先选择 FLOAT_VECTOR（稠密向量）的场景
   * 向量维度适中（通常在 128~2048 维之间），且非零值占比高（例如超过 30%）。
   * 由深度学习模型生成，如：
      + 文本：BERT、GPT、Sentence-BERT 等预训练模型的输出（固定维度，几乎所有维度都有非零值）。
      + 图像：ResNet、VGG 等模型的特征向量（稠密表示图像特征）。
      + 音频：MFCC 特征、语音识别模型的嵌入向量。
   * 需要高频相似度计算（如实时检索），稠密向量的紧凑存储形式能提升计算效率。
2. 优先选择 SPARSE_FLOAT_VECTOR（稀疏向量）的场景
   * 向量维度极高（通常超过 1 万维），且非零值占比极低（例如低于 1%）。
   * 由传统统计或离散特征生成，如：
      + 文本：TF-IDF、词袋模型（BoW）、One-Hot 编码（维度等于词汇表大小，通常几十万维，仅少数词对应非零值）。
      + 推荐系统：用户 - 物品交互矩阵（用户数 / 物品数极大，大多数用户未交互过大多数物品）。
      + 特征工程：高维离散特征（如类别型特征的哈希编码）。
   * 对存储空间敏感，稀疏向量通过只存储非零值，可大幅减少存储开销（例如 100 万维的稀疏向量，若仅 100 个非零值，存储量远小于稠密向量）。
3. 特殊情况的判断
   * 如果向量维度超过 2048 但非零值较多（如 1 万维中 30% 是非零值），此时稀疏向量的存储优势不明显，反而可能因索引开销降低效率，建议用稠密向量。
   * 如果是混合特征（部分稠密 + 部分稀疏），可拆分存储（稠密部分用 FLOAT_VECTOR，稀疏部分用 SPARSE_FLOAT_VECTOR），或通过模型将稀疏特征转为稠密表示（如通过嵌入层）。

#### 度量类型
相似度量用于衡量向量之间的相似性。选择合适的距离度量有助于显著提高分类和聚类性能。 
目前，Milvus 支持这些类型的相似性度量：欧氏距离 (L2)、内积 (IP)、余弦相似度 (COSINE)、JACCARD,HAMMING 和BM25 （专门为稀疏向量的全文检索而设计）。

1. 欧氏距离（L2）
   * 公式：$L2 = d(a,b) = d(b,a) = \sqrt{\sum_{i=1}^n (a_i - b_i)^2}$
   * 解释：欧氏距离衡量向量空间中两点之间的直线距离。它是最常用的距离度量，因为它简单直接，且在许多应用中都有良好的性能。
   * 应用场景：欧氏距离常用于需要测量向量之间的绝对距离的场景，如图像识别、语音识别等。
   * 注意事项：欧氏距离对向量的缩放敏感，因此在使用前需要对向量进行归一化处理。
2. 内积（IP）
   * 公式：$IP = P(A,B) = \sum_{i=1}^n a_i \cdot b_i$
   * 解释：内积衡量向量的投影重叠度，与余弦相似度效果一致。它在归一化向量后，直接计算向量的点积，无需计算向量长度，因此计算效率高。
   * 应用场景：内积常用于文本分类、推荐系统等场景，因为它可以直接计算文本向量的相似度，而无需计算文本的长度。
   * 注意事项：内积不考虑向量的方向，仅考虑向量的重叠度。因此，在使用前需要确保向量已经归一化，即向量的长度为 1。
3. 余弦相似度（COSINE）
   * 公式：$Cos(A,B) = \frac{\sum_{i=1}^n (a_i \cdot b_i)}{\sqrt{\sum_{i=1}^n (a_i^2)} \sqrt{\sum_{i=1}^n (b_i^2)}}$
   * 解释：余弦相似度衡量向量的方向相似度，取值范围 [-1, 1]，1 表示完全相似，-1 表示完全不相似。
   * 应用场景：余弦相似度常用于文本相似度计算、推荐系统等场景，因为它可以直接计算文本向量的相似度，而无需计算文本的长度。
   * 注意事项：余弦相似度不考虑向量的长度，仅考虑向量的方向。因此，在使用前需要确保向量已经归一化，即向量的长度为 1。
4. JACCARD 距离
   * 公式：$J(A,B) = \frac{|A \cap B|}{|A|+|B|-|A \cap B|}$
   * 解释：JACCARD 距离衡量向量的交集与并集的比例，取值范围 [0, 1]，0 表示完全不相似，1 表示完全相似。
   * 应用场景：JACCARD 距离常用于文本分类、推荐系统等场景，因为它可以直接计算文本向量的相似度，而无需计算文本的长度。
   * 注意事项：JACCARD 距离仅考虑向量的交集与并集，不考虑向量的方向。因此，在使用前需要确保向量已经归一化，即向量的长度为 1。
5. MHJACCARD 距离
   MinHash Jaccard(MHJACCARD) 是一种度量类型，用于在大型集合（如文档单词集、用户标签集或基因组 k-mer 集）上进行高效、近似的相似性搜索。MHJACCARD 不直接比较原始集，而是比较MinHash 签名，MinHash 签名是专为高效估计 Jaccard 相似性而设计的紧凑表示法。这种方法比计算精确的 Jaccard 相似性要快得多，尤其适用于大规模或高维场景。
   * 适用向量类型：BINARY_VECTOR，其中每个向量存储一个 MinHash 签名。每个元素都对应于应用于原始集合的一个独立哈希函数下的最小哈希值
   * 距离定义 MHJACCARD 衡量两个 MinHash 签名中匹配位置的数量。匹配率越高，说明底层集越相似。
   * Milvus 报告：距离 = 1 - 估计相似度（匹配率）距离值从 0 到 1 不等：0 表示完全不相似，1 表示完全相似。
6. HAMMING 距离
   
   HAMMING 距离测量二进制数据字符串。两个长度相等的字符串之间的距离是比特不同的比特位置数
   
   例如，假设有两个字符串：1101 1001 和 1001 1101
      
   1011001 ⊕ 10011101 = 01000100.由于其中包含两个 1，因此 HAMMING 距离 d (11011001, 10011101) = 2。
7. BM25 相似性

   BM25 是一种广泛使用的文本相关性测量方法，专门用于全文检索。它结合了以下三个关键因素：
   * 术语频率 (TF)：衡量术语在文档中出现的频率。虽然较高的频率通常表示较高的重要性，但 BM25 使用饱和参数 k 1 来防止过于频繁的术语主导相关性得分。
   * 反向文档频率（IDF）：反映术语在整个语料库中的重要性。在较少文档中出现的术语会获得较高的 IDF 值，这表明其对相关性的贡献更大。
   * 文档长度归一化：较长的文档由于包含较多的术语，往往得分较高。BM25 通过对文档长度进行归一化处理来减轻这种偏差，参数b 控制这种归一化处理的强度。

   BM25 评分的计算方法如下：
   * 公式：$BM25 = score(D,Q) = \sum_{i=1}^n (IDF(q_i) \cdot \frac {TF(q_i,D) \cdot (k_1 + 1))} { TF(q_i, D) + k_1 \times (1 - b + b \cdot \frac{|D|}{avg\_doc\_len})}$

   参数描述：
   * Q：用户提供的查询文本。
   * D：被评估的文档。
   * $TF(q_i , D)$：术语频率，表示术语 $q_i$ 在文档 D 中出现的频率。
   * $IDF(q_i)$：反向文档频率，表示术语 $q_i$ 在整个语料库中的重要性。
     $IDF(q_i)=log(\frac{N-n(q_i)+0.5}{n(q_i)+0.5}+1)$
     其中N为文档总数，$n(q_i)$为包含术语q的文档数。
   * $|D|$：文档 D 的长度，即文档中包含的术语数量。
   * $avg\_doc\_len$：平均文档长度，即所有文档中包含的术语数量的平均值。
   * k ：控制词频对评分的影响。数值越大，词频越重要。典型的范围是 [1.2, 2.0]，而 Milvus 允许的范围是 [0, 3]。
   * b ：控制文档长度归一化的影响。范围从 0 到 1。当值为 0 时，不进行归一化处理；当值为 1 时，进行完全归一化处理。数值越大，文档长度归一化越重要。典型的范围是 [0.75, 1.2]，而 Milvus 允许的范围是 [0, 1]。
